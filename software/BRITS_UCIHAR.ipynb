{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All experiments using our approach for various missingness rates and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulate\n",
    "import baseline\n",
    "import dataload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import pypots\n",
    "from pypots.data import load_specific_dataset, mcar, masked_fill\n",
    "from pypots.imputation import BRITS\n",
    "from pypots.utils.metrics import cal_mae, cal_rmse, cal_mre\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import pypots\n",
    "from pypots.data import load_specific_dataset, mcar, masked_fill\n",
    "from pypots.imputation import BRITS\n",
    "import torch\n",
    "from pypots.utils.metrics import cal_mae\n",
    "from pypots.utils.metrics import cal_binary_classification_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path :  C:/Users/Ronak/Desktop/Data_imputation/PyPOTS-main/ece209as_project-main/ece209as_project-main/data/UCI_HAR_Dataset/train/Inertial_Signals/\n",
      "File Path :  C:/Users/Ronak/Desktop/Data_imputation/PyPOTS-main/ece209as_project-main/ece209as_project-main/data/UCI_HAR_Dataset/test/Inertial_Signals/\n"
     ]
    }
   ],
   "source": [
    "dict = dataload.uci()\n",
    "X = dict[\"X\"]\n",
    "y = dict[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.1, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "# MAR - logistic\n",
    "dict_MAR = simulate.simulate_nan(X, 0.1, \"MAR\")\n",
    "X_intact_MAR = dict_MAR['X_init']\n",
    "X_missing_MAR = dict_MAR['X_incomp']\n",
    "X_mask_MAR = dict_MAR['mask']\n",
    "\n",
    "# MNAR - logistic\n",
    "dict_MAR = simulate.simulate_nan(X, 0.1, \"MNAR\", opt=\"logistic\")\n",
    "X_intact_MNAR = dict_MAR['X_init']\n",
    "X_missing_MNAR = dict_MAR['X_incomp']\n",
    "X_mask_MNAR = dict_MAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR, X_intact_MAR, X_intact_MNAR]\n",
    "X_missing = [X_missing_MCAR, X_missing_MAR, X_missing_MNAR]\n",
    "X_mask = [X_mask_MCAR, X_mask_MAR, X_mask_MNAR]\n",
    "missingness = [\"MCAR\", \"MAR\", \"MNAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "    \n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "      <th>MAR mae</th>\n",
       "      <th>MAR rmse</th>\n",
       "      <th>MAR mre</th>\n",
       "      <th>MNAR mae</th>\n",
       "      <th>MNAR rmse</th>\n",
       "      <th>MNAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.123852</td>\n",
       "      <td>0.22107</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.128857</td>\n",
       "      <td>0.225485</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.123321</td>\n",
       "      <td>0.217309</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre   MAR mae  MAR rmse  MAR mre  MNAR mae  \\\n",
       "BRITS  0.123852    0.22107     0.511  0.128857  0.225485    0.534  0.123321   \n",
       "\n",
       "       MNAR rmse  MNAR mre  \n",
       "BRITS   0.217309     0.509  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"har_weights.h5\", monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.3731 - accuracy: 0.8553 - val_loss: 0.1764 - val_accuracy: 0.9240\n",
      "Epoch 2/50\n",
      "365/365 [==============================] - 5s 13ms/step - loss: 0.1505 - accuracy: 0.9388 - val_loss: 0.1432 - val_accuracy: 0.9553\n",
      "Epoch 3/50\n",
      "365/365 [==============================] - 6s 17ms/step - loss: 0.1310 - accuracy: 0.9459 - val_loss: 0.1485 - val_accuracy: 0.9313\n",
      "Epoch 4/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1231 - accuracy: 0.9485 - val_loss: 0.1117 - val_accuracy: 0.9473\n",
      "Epoch 5/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1178 - accuracy: 0.9510 - val_loss: 0.1063 - val_accuracy: 0.9547\n",
      "Epoch 6/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1112 - accuracy: 0.9538 - val_loss: 0.1436 - val_accuracy: 0.9500\n",
      "Epoch 7/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1229 - accuracy: 0.9499 - val_loss: 0.1012 - val_accuracy: 0.9600\n",
      "Epoch 8/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1025 - accuracy: 0.9553 - val_loss: 0.1269 - val_accuracy: 0.9447\n",
      "Epoch 9/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1050 - accuracy: 0.9589 - val_loss: 0.1094 - val_accuracy: 0.9540\n",
      "Epoch 10/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1049 - accuracy: 0.9579 - val_loss: 0.1023 - val_accuracy: 0.9613\n",
      "Epoch 11/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0901 - accuracy: 0.9600 - val_loss: 0.0919 - val_accuracy: 0.9660\n",
      "Epoch 12/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0866 - accuracy: 0.9631 - val_loss: 0.0860 - val_accuracy: 0.9593\n",
      "Epoch 13/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1065 - accuracy: 0.9581 - val_loss: 0.1032 - val_accuracy: 0.9613\n",
      "Epoch 14/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0886 - accuracy: 0.9644 - val_loss: 0.0897 - val_accuracy: 0.9640\n",
      "Epoch 15/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0745 - accuracy: 0.9692 - val_loss: 0.0865 - val_accuracy: 0.9620\n",
      "Epoch 16/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0709 - accuracy: 0.9712 - val_loss: 0.0759 - val_accuracy: 0.9707\n",
      "Epoch 17/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0733 - accuracy: 0.9715 - val_loss: 0.0877 - val_accuracy: 0.9647\n",
      "Epoch 18/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0742 - accuracy: 0.9710 - val_loss: 0.0904 - val_accuracy: 0.9713\n",
      "Epoch 19/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0647 - accuracy: 0.9722 - val_loss: 0.1076 - val_accuracy: 0.9640\n",
      "Epoch 20/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0543 - accuracy: 0.9763 - val_loss: 0.0842 - val_accuracy: 0.9693\n",
      "Epoch 21/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0512 - accuracy: 0.9779 - val_loss: 0.0742 - val_accuracy: 0.9767\n",
      "Epoch 22/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0693 - accuracy: 0.9740 - val_loss: 0.0955 - val_accuracy: 0.9707\n",
      "Epoch 23/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0611 - accuracy: 0.9764 - val_loss: 0.0885 - val_accuracy: 0.9673\n",
      "Epoch 24/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0427 - accuracy: 0.9816 - val_loss: 0.0772 - val_accuracy: 0.9787\n",
      "Epoch 25/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0482 - accuracy: 0.9801 - val_loss: 0.0778 - val_accuracy: 0.9747\n",
      "Epoch 26/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0506 - accuracy: 0.9800 - val_loss: 0.1031 - val_accuracy: 0.9713\n",
      "Epoch 27/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0371 - accuracy: 0.9849 - val_loss: 0.0837 - val_accuracy: 0.9720\n",
      "Epoch 28/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0333 - accuracy: 0.9877 - val_loss: 0.0803 - val_accuracy: 0.9773\n",
      "Epoch 29/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0353 - accuracy: 0.9852 - val_loss: 0.0827 - val_accuracy: 0.9753\n",
      "Epoch 30/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0375 - accuracy: 0.9844 - val_loss: 0.0779 - val_accuracy: 0.9813\n",
      "Epoch 31/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0638 - accuracy: 0.9788 - val_loss: 0.0973 - val_accuracy: 0.9733\n",
      "Epoch 32/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0358 - accuracy: 0.9864 - val_loss: 0.0746 - val_accuracy: 0.9780\n",
      "Epoch 33/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0258 - accuracy: 0.9893 - val_loss: 0.0739 - val_accuracy: 0.9800\n",
      "Epoch 34/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0296 - accuracy: 0.9893 - val_loss: 0.0752 - val_accuracy: 0.9813\n",
      "Epoch 35/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0308 - accuracy: 0.9901 - val_loss: 0.0798 - val_accuracy: 0.9793\n",
      "Epoch 36/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0219 - accuracy: 0.9912 - val_loss: 0.0824 - val_accuracy: 0.9793\n",
      "Epoch 37/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0217 - accuracy: 0.9919 - val_loss: 0.0840 - val_accuracy: 0.9760\n",
      "Epoch 38/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0362 - accuracy: 0.9873 - val_loss: 0.2079 - val_accuracy: 0.9633\n",
      "Epoch 39/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0325 - accuracy: 0.9907 - val_loss: 0.0684 - val_accuracy: 0.9827\n",
      "Epoch 40/50\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.0847 - val_accuracy: 0.9827\n",
      "Epoch 41/50\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0245 - accuracy: 0.9919 - val_loss: 0.0988 - val_accuracy: 0.9727\n",
      "Epoch 42/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0286 - accuracy: 0.9919 - val_loss: 0.0977 - val_accuracy: 0.9820\n",
      "Epoch 43/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0173 - accuracy: 0.9941 - val_loss: 0.0784 - val_accuracy: 0.9807\n",
      "Epoch 44/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0167 - accuracy: 0.9938 - val_loss: 0.0865 - val_accuracy: 0.9807\n",
      "Epoch 45/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0270 - accuracy: 0.9905 - val_loss: 0.0882 - val_accuracy: 0.9780\n",
      "Epoch 46/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0120 - accuracy: 0.9949 - val_loss: 0.1314 - val_accuracy: 0.9827\n",
      "Epoch 47/50\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.1224 - val_accuracy: 0.9760\n",
      "Epoch 48/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0197 - accuracy: 0.9936 - val_loss: 0.1179 - val_accuracy: 0.9720\n",
      "Epoch 49/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0169 - accuracy: 0.9934 - val_loss: 0.1090 - val_accuracy: 0.9787\n",
      "Epoch 50/50\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.0127 - accuracy: 0.9951 - val_loss: 0.1166 - val_accuracy: 0.9813\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.9833\n",
      "[INFO] loss=0.0570, accuracy: 98.3333%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0281 - accuracy: 0.9465\n",
      "HAR - CNN Test Accuracy: 91.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification performance on MAR type missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[1],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"har_weights.h5\", monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0381 - accuracy: 0.8901\n",
      "HAR - CNN Test Accuracy: 86.62%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification performance on MNAR type missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[2],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"har_weights.h5\", monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0321 - accuracy: 0.9111\n",
      "HAR - CNN Test Accuracy: 89.85%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.2, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.103246</td>\n",
       "      <td>0.184325</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.103246   0.184325     0.957"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"har_weights.h5\", monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.3869 - accuracy: 0.8456 - val_loss: 0.1847 - val_accuracy: 0.9360\n",
      "Epoch 2/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1575 - accuracy: 0.9388 - val_loss: 0.1424 - val_accuracy: 0.9493\n",
      "Epoch 3/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1286 - accuracy: 0.9481 - val_loss: 0.1268 - val_accuracy: 0.9560\n",
      "Epoch 4/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1202 - accuracy: 0.9505 - val_loss: 0.1343 - val_accuracy: 0.9507\n",
      "Epoch 5/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1125 - accuracy: 0.9559 - val_loss: 0.1103 - val_accuracy: 0.9573\n",
      "Epoch 6/50\n",
      "365/365 [==============================] - 4s 10ms/step - loss: 0.1163 - accuracy: 0.9542 - val_loss: 0.2458 - val_accuracy: 0.9173\n",
      "Epoch 7/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1301 - accuracy: 0.9477 - val_loss: 0.1171 - val_accuracy: 0.9453\n",
      "Epoch 8/50\n",
      "365/365 [==============================] - 5s 13ms/step - loss: 0.1055 - accuracy: 0.9564 - val_loss: 0.1137 - val_accuracy: 0.9500\n",
      "Epoch 9/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1029 - accuracy: 0.9564 - val_loss: 0.1007 - val_accuracy: 0.9580\n",
      "Epoch 10/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1115 - accuracy: 0.9526 - val_loss: 0.1053 - val_accuracy: 0.9553\n",
      "Epoch 11/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0977 - accuracy: 0.9584 - val_loss: 0.1025 - val_accuracy: 0.9587\n",
      "Epoch 12/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0931 - accuracy: 0.9601 - val_loss: 0.1064 - val_accuracy: 0.9567\n",
      "Epoch 13/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0953 - accuracy: 0.9607 - val_loss: 0.1214 - val_accuracy: 0.9513\n",
      "Epoch 14/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0964 - accuracy: 0.9603 - val_loss: 0.0994 - val_accuracy: 0.9600\n",
      "Epoch 15/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0983 - accuracy: 0.9601 - val_loss: 0.1329 - val_accuracy: 0.9520\n",
      "Epoch 16/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0943 - accuracy: 0.9633 - val_loss: 0.0899 - val_accuracy: 0.9693\n",
      "Epoch 17/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0814 - accuracy: 0.9652 - val_loss: 0.0908 - val_accuracy: 0.9747\n",
      "Epoch 18/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0864 - accuracy: 0.9636 - val_loss: 0.0858 - val_accuracy: 0.9687\n",
      "Epoch 19/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0684 - accuracy: 0.9710 - val_loss: 0.0950 - val_accuracy: 0.9713\n",
      "Epoch 20/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0669 - accuracy: 0.9722 - val_loss: 0.1189 - val_accuracy: 0.9580\n",
      "Epoch 21/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0630 - accuracy: 0.9737 - val_loss: 0.0823 - val_accuracy: 0.9733\n",
      "Epoch 22/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0836 - accuracy: 0.9677 - val_loss: 0.0754 - val_accuracy: 0.9727\n",
      "Epoch 23/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0744 - accuracy: 0.9696 - val_loss: 0.0803 - val_accuracy: 0.9713\n",
      "Epoch 24/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0604 - accuracy: 0.9744 - val_loss: 0.1112 - val_accuracy: 0.9673\n",
      "Epoch 25/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0638 - accuracy: 0.9740 - val_loss: 0.0801 - val_accuracy: 0.9707\n",
      "Epoch 26/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0502 - accuracy: 0.9803 - val_loss: 0.0803 - val_accuracy: 0.9707\n",
      "Epoch 27/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0563 - accuracy: 0.9796 - val_loss: 0.0968 - val_accuracy: 0.9700\n",
      "Epoch 28/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0678 - accuracy: 0.9781 - val_loss: 0.1199 - val_accuracy: 0.9633\n",
      "Epoch 29/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0443 - accuracy: 0.9814 - val_loss: 0.0981 - val_accuracy: 0.9673\n",
      "Epoch 30/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0510 - accuracy: 0.9805 - val_loss: 0.1009 - val_accuracy: 0.9700\n",
      "Epoch 31/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0436 - accuracy: 0.9819 - val_loss: 0.0794 - val_accuracy: 0.9693\n",
      "Epoch 32/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0475 - accuracy: 0.9801 - val_loss: 0.0936 - val_accuracy: 0.9673\n",
      "Epoch 33/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0715 - accuracy: 0.9756 - val_loss: 0.0998 - val_accuracy: 0.9713\n",
      "Epoch 34/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0378 - accuracy: 0.9840 - val_loss: 0.0856 - val_accuracy: 0.9760\n",
      "Epoch 35/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0327 - accuracy: 0.9878 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 36/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0321 - accuracy: 0.9870 - val_loss: 0.0944 - val_accuracy: 0.9700\n",
      "Epoch 37/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0321 - accuracy: 0.9860 - val_loss: 0.1358 - val_accuracy: 0.9733\n",
      "Epoch 38/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0346 - accuracy: 0.9863 - val_loss: 0.0882 - val_accuracy: 0.9760\n",
      "Epoch 39/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0276 - accuracy: 0.9888 - val_loss: 0.0946 - val_accuracy: 0.9720\n",
      "Epoch 40/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0502 - accuracy: 0.9867 - val_loss: 0.1311 - val_accuracy: 0.9687\n",
      "Epoch 41/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0388 - accuracy: 0.9864 - val_loss: 0.0934 - val_accuracy: 0.9720\n",
      "Epoch 42/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0293 - accuracy: 0.9885 - val_loss: 0.1314 - val_accuracy: 0.9693\n",
      "Epoch 43/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0246 - accuracy: 0.9900 - val_loss: 0.1041 - val_accuracy: 0.9800\n",
      "Epoch 44/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0215 - accuracy: 0.9911 - val_loss: 0.1430 - val_accuracy: 0.9720\n",
      "Epoch 45/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0646 - accuracy: 0.9811 - val_loss: 0.0910 - val_accuracy: 0.9813\n",
      "Epoch 46/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0228 - accuracy: 0.9912 - val_loss: 0.0841 - val_accuracy: 0.9787\n",
      "Epoch 47/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.0849 - val_accuracy: 0.9793\n",
      "Epoch 48/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0176 - accuracy: 0.9929 - val_loss: 0.0821 - val_accuracy: 0.9827\n",
      "Epoch 49/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0273 - accuracy: 0.9900 - val_loss: 0.0790 - val_accuracy: 0.9760\n",
      "Epoch 50/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 0.0790 - val_accuracy: 0.9813\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9853\n",
      "[INFO] loss=0.0491, accuracy: 98.5333%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0311 - accuracy: 0.9285\n",
      "HAR - CNN Test Accuracy: 90.73%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 30% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.3, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.095643</td>\n",
       "      <td>0.186893</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.095643   0.186893     0.997"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.3758 - accuracy: 0.8493 - val_loss: 0.1955 - val_accuracy: 0.9220\n",
      "Epoch 2/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1583 - accuracy: 0.9346 - val_loss: 0.1402 - val_accuracy: 0.9420\n",
      "Epoch 3/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1245 - accuracy: 0.9475 - val_loss: 0.1221 - val_accuracy: 0.9580\n",
      "Epoch 4/50\n",
      "365/365 [==============================] - 5s 13ms/step - loss: 0.1150 - accuracy: 0.9525 - val_loss: 0.1313 - val_accuracy: 0.9513\n",
      "Epoch 5/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.1148 - accuracy: 0.9529 - val_loss: 0.1224 - val_accuracy: 0.9527\n",
      "Epoch 6/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1159 - accuracy: 0.9519 - val_loss: 0.1212 - val_accuracy: 0.9467\n",
      "Epoch 7/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.1058 - accuracy: 0.9566 - val_loss: 0.1116 - val_accuracy: 0.9620\n",
      "Epoch 8/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.1164 - accuracy: 0.9544 - val_loss: 0.1088 - val_accuracy: 0.9480\n",
      "Epoch 9/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0966 - accuracy: 0.9592 - val_loss: 0.1107 - val_accuracy: 0.9613\n",
      "Epoch 10/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0983 - accuracy: 0.9592 - val_loss: 0.0886 - val_accuracy: 0.9640\n",
      "Epoch 11/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0867 - accuracy: 0.9637 - val_loss: 0.0835 - val_accuracy: 0.9653\n",
      "Epoch 12/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0841 - accuracy: 0.9663 - val_loss: 0.1643 - val_accuracy: 0.9467\n",
      "Epoch 13/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0804 - accuracy: 0.9663 - val_loss: 0.0825 - val_accuracy: 0.9667\n",
      "Epoch 14/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0777 - accuracy: 0.9697 - val_loss: 0.0774 - val_accuracy: 0.9740\n",
      "Epoch 15/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0664 - accuracy: 0.9729 - val_loss: 0.0871 - val_accuracy: 0.9620\n",
      "Epoch 16/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0652 - accuracy: 0.9744 - val_loss: 0.1280 - val_accuracy: 0.9600\n",
      "Epoch 17/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0730 - accuracy: 0.9729 - val_loss: 0.0971 - val_accuracy: 0.9673\n",
      "Epoch 18/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0903 - accuracy: 0.9704 - val_loss: 0.0633 - val_accuracy: 0.9780\n",
      "Epoch 19/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0548 - accuracy: 0.9767 - val_loss: 0.0702 - val_accuracy: 0.9800\n",
      "Epoch 20/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0485 - accuracy: 0.9794 - val_loss: 0.0699 - val_accuracy: 0.9807\n",
      "Epoch 21/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0473 - accuracy: 0.9814 - val_loss: 0.0645 - val_accuracy: 0.9787\n",
      "Epoch 22/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0485 - accuracy: 0.9799 - val_loss: 0.0991 - val_accuracy: 0.9660\n",
      "Epoch 23/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0511 - accuracy: 0.9819 - val_loss: 0.0755 - val_accuracy: 0.9733\n",
      "Epoch 24/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0393 - accuracy: 0.9845 - val_loss: 0.0645 - val_accuracy: 0.9820\n",
      "Epoch 25/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0427 - accuracy: 0.9852 - val_loss: 0.0703 - val_accuracy: 0.9760\n",
      "Epoch 26/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.0677 - val_accuracy: 0.9833\n",
      "Epoch 27/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0344 - accuracy: 0.9864 - val_loss: 0.0870 - val_accuracy: 0.9767\n",
      "Epoch 28/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0359 - accuracy: 0.9860 - val_loss: 0.2167 - val_accuracy: 0.9480\n",
      "Epoch 29/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0657 - accuracy: 0.9816 - val_loss: 0.0705 - val_accuracy: 0.9773\n",
      "Epoch 30/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0306 - accuracy: 0.9873 - val_loss: 0.0833 - val_accuracy: 0.9733\n",
      "Epoch 31/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0392 - accuracy: 0.9866 - val_loss: 0.0615 - val_accuracy: 0.9813\n",
      "Epoch 32/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0291 - accuracy: 0.9889 - val_loss: 0.0709 - val_accuracy: 0.9820\n",
      "Epoch 33/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0240 - accuracy: 0.9910 - val_loss: 0.0653 - val_accuracy: 0.9840\n",
      "Epoch 34/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0317 - accuracy: 0.9885 - val_loss: 0.0783 - val_accuracy: 0.9827\n",
      "Epoch 35/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0294 - accuracy: 0.9892 - val_loss: 0.0945 - val_accuracy: 0.9767\n",
      "Epoch 36/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0231 - accuracy: 0.9905 - val_loss: 0.0695 - val_accuracy: 0.9840\n",
      "Epoch 37/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 0.1342 - val_accuracy: 0.9627\n",
      "Epoch 38/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0355 - accuracy: 0.9879 - val_loss: 0.0612 - val_accuracy: 0.9827\n",
      "Epoch 39/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0218 - accuracy: 0.9915 - val_loss: 0.0717 - val_accuracy: 0.9847\n",
      "Epoch 40/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.0634 - val_accuracy: 0.9840\n",
      "Epoch 41/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.0686 - val_accuracy: 0.9840\n",
      "Epoch 42/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.0104 - accuracy: 0.9959 - val_loss: 0.0819 - val_accuracy: 0.9853\n",
      "Epoch 43/50\n",
      "365/365 [==============================] - 5s 13ms/step - loss: 0.0155 - accuracy: 0.9958 - val_loss: 0.0696 - val_accuracy: 0.9840\n",
      "Epoch 44/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0212 - accuracy: 0.9938 - val_loss: 0.1015 - val_accuracy: 0.9793\n",
      "Epoch 45/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0111 - accuracy: 0.9959 - val_loss: 0.0823 - val_accuracy: 0.9827\n",
      "Epoch 46/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.0178 - accuracy: 0.9944 - val_loss: 0.0919 - val_accuracy: 0.9833\n",
      "Epoch 47/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 0.1449 - val_accuracy: 0.9773\n",
      "Epoch 48/50\n",
      "365/365 [==============================] - 4s 12ms/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.1137 - val_accuracy: 0.9680\n",
      "Epoch 49/50\n",
      "365/365 [==============================] - 5s 12ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.1010 - val_accuracy: 0.9860\n",
      "Epoch 50/50\n",
      "365/365 [==============================] - 4s 11ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.1307 - val_accuracy: 0.9820\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9873\n",
      "[INFO] loss=0.0510, accuracy: 98.7333%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0411 - accuracy: 0.8455\n",
      "HAR - CNN Test Accuracy: 82.62%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 40% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.4, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.113654</td>\n",
       "      <td>0.189329</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.113654   0.189329     0.999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.0602 - accuracy: 0.8121\n",
      "HAR - CNN Test Accuracy: 76.86%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 50% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.5, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.113654</td>\n",
       "      <td>0.19139</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.113654    0.19139       1.1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.112 - accuracy: 0.7001\n",
      "HAR - CNN Test Accuracy: 68.43%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 60% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.6, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.115624</td>\n",
       "      <td>0.198277</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.115624   0.198277      11.3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.112 - accuracy: 0.6422\n",
      "HAR - CNN Test Accuracy: 59.14%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 70% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.7, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.115988</td>\n",
       "      <td>0.199835</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.115988   0.199835      1.23"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.1912 - accuracy: 0.4617\n",
      "HAR - CNN Test Accuracy: 48.22%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 80% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.8, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.118903</td>\n",
       "      <td>0.21341</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.118903    0.21341      1.83"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.1912 - accuracy: 0.4617\n",
      "HAR - CNN Test Accuracy: 43.86%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR BRITS with Missingness at 90% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCAR\n",
    "dict_MCAR = simulate.simulate_nan(X, 0.8, \"MCAR\")\n",
    "X_intact_MCAR = dict_MCAR['X_init']\n",
    "X_missing_MCAR = dict_MCAR['X_incomp']\n",
    "X_mask_MCAR = dict_MCAR['mask']\n",
    "\n",
    "X_intact = [X_intact_MCAR]\n",
    "X_missing = [X_missing_MCAR]\n",
    "X_mask = [X_mask_MCAR]\n",
    "missingness = [\"MCAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uci_imputation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "imputated_data = []\n",
    "\n",
    "for i in range(1):\n",
    "    brits = BRITS(n_steps=128, n_features=9,rnn_hidden_size=64, batch_size=256, epochs=100,device=torch.device('cuda:0'))\n",
    "    brits.fit(X_missing[i])\n",
    "    imputation = brits.impute(X_missing[i])\n",
    "    imputated_data.append(imputation)\n",
    "    res.append(cal_mae(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_rmse(imputation, X_intact[i], X_mask[i]))\n",
    "    res.append(cal_mre(imputation, X_intact[i], X_mask[i]))\n",
    "\n",
    "result_uci_imputation.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCAR mae</th>\n",
       "      <th>MCAR rmse</th>\n",
       "      <th>MCAR mre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRITS</th>\n",
       "      <td>0.123216</td>\n",
       "      <td>0.23548</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MCAR mae  MCAR rmse  MCAR mre\n",
       "BRITS  0.123216    0.23548      2.13"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = [\"mae\", \"rmse\", \"mre\"]\n",
    "cols = []\n",
    "\n",
    "for i in missingness[:1]:\n",
    "    for j in errors:\n",
    "        cols.append(i+\" \"+j)\n",
    "\n",
    "methods = [\"BRITS\"]\n",
    "result_uci_imputation_df = pd.DataFrame(result_uci_imputation, columns = cols, index=methods)\n",
    "result_uci_imputation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (7299, 128, 9)\n",
      "Y_train.shape :  (7299, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n",
      "X_test.shape :  (1500, 128, 9)\n",
      "Y_test.shape :  (1500, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_intact[0],y,test_size=1500/len(y),random_state=42,stratify=y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=1500/len(Y_train),random_state=42,stratify=Y_train)\n",
    "# zero-offset class values\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1\n",
    "Y_val = Y_val - 1\n",
    "def to_categorical(y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "# one hot encode y\n",
    "Y_train = np.squeeze(to_categorical(Y_train),axis=1)\n",
    "Y_test = np.squeeze(to_categorical(Y_test),axis=1)\n",
    "Y_val = np.squeeze(to_categorical(Y_val),axis=1)\n",
    "\n",
    "\n",
    "print('X_train.shape : ', X_train.shape)\n",
    "print('Y_train.shape : ', Y_train.shape)\n",
    "print('X_test.shape : ', X_test.shape)\n",
    "print('Y_test.shape : ', Y_test.shape)\n",
    "print('X_test.shape : ', X_val.shape)\n",
    "print('Y_test.shape : ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "epochs = 50\n",
    "batch_size = 20\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "n_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding = 'same'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "(loss, accuracy) = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=verbose)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 2s 3ms/step - loss: 0.1912 - accuracy: 0.4617\n",
      "HAR - CNN Test Accuracy: 39.45%\n"
     ]
    }
   ],
   "source": [
    "imp_y = np.squeeze(to_categorical(y-1),axis=1)\n",
    "scores = model.evaluate(imputated_data[0], imp_y, batch_size = batch_size, verbose = verbose)\n",
    "print(\"HAR - CNN Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f025751ccf0c389e1b30a94f28ae2e988dabc5f002f93e8d8c9aff6e67cedc67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
